{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressao logistica\n",
    "![](imagens/regressao.png)\n",
    "### Perceptron\n",
    "Assistir o video aula 2.4 - Redes Neurais\n",
    "![](imagens/rede1.png)\n",
    "![](imagens/rede2.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percetron E\n",
    "![](imagens/perceptron_e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You got 1 wrong.  Keep trying!\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                    -4                    0          Yes\n",
      "      0          1                    -6                    0          Yes\n",
      "      1          0                    -2                    0          Yes\n",
      "      1          1                    -4                    0           No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 2\n",
    "weight2 = -2\n",
    "bias = -4\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percetron OU\n",
    "![](imagens/perceptron_ou 2.png)\n",
    "\n",
    "![](imagens/perceptron_ou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                    -1                    0          Yes\n",
      "      0          1                     0                    1          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                     1                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1\n",
    "weight2 = 1 \n",
    "bias = -1\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron do NÃO\n",
    "Ao contrário dos outros perceptrons que estudamos, as operações com NÃO agem apenas sobre um input. A operação retorna 0 caso o input seja 1 e 1 caso o input seja 0. As outras entradas dadas ao perceptron são ignoradas.\n",
    "Neste teste, você ajustará os pesos e o viés para os valores que calculam a operação NÃO no segundo input e ignoram o primeiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                     0                    1          Yes\n",
      "      0          1                    -1                    0          Yes\n",
      "      1          0                     0                    1          Yes\n",
      "      1          1                    -1                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0\n",
    "weight2 = -1\n",
    "bias = 0\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes neurais\n",
    "![](imagens/rede3.png)\n",
    "<center>Diagrama de uma rede neural simples. Círculos são unidades e caixas são operações</center>\n",
    "\n",
    "A parte legal dessa arquitetura, e o que faz as redes neurais possíveis, é que a função de ativação, f(h) pode ser qualquer função, não apenas a função degrau que usado até agora pouco.\n",
    "\n",
    "Por exemplo, caso f(h) = h, o output será o mesmo que o input. Agora o output da rede é y = sum{i} w{i}x{i} + b\n",
    "\n",
    "Essa equação deveria ser familiar para você, pois é a mesma do modelo de regressão linear!\n",
    "\n",
    "Outras funções de ativação comuns são a função logística (também chamada de sigmóide), tanh e a função softmax. Nós iremos trabalhar principalmente com a função sigmóide pelo resto dessa aula: sigmoide(x) = 1/(1+e^{-x})\n",
    "\n",
    "![](imagens/sigmoide.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.432907095035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # TODO: Implement sigmoid function\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "# TODO: Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendendo os pesos\n",
    "\n",
    "### Soma quadrática dos erros (SQE):\n",
    "\n",
    "Vimos como podemos usar os perceptrons para operações E e XOR, mas nós definimos os pesos manualmente. E se a operação que desejamos fazer for algo como prever a admissão na faculdade, mas não sabemos os pesos corretos? Para isso é necessário aprender os pesos a partir de dados, e usar esses pesos para fazer previsões.\n",
    "\n",
    "Para entender como iremos encontrar esses pesos, comece pensando no objetivo. Queremos que a rede faça previsões o mais próximas possíveis dos valores reais. Para medir isso, precisamos de uma medida de quão distantes as previsões estão da verdade, ou seja, um método de calcular o erro. Uma medida comum é a soma quadrática dos erros (SQE):\n",
    "![](imagens/sqe1.png)\n",
    "\n",
    "onde y^ é a previsão e y é o valor verdadeiro e então faz a soma interna de todos os outputs em j assim como se faz a soma interna de todos os dados u.\n",
    "![](imagens/sqe2.png)\n",
    "\n",
    "Em primeiro lugar, a soma interna de j. Essa variável j representa as unidades de output da rede. Então esta soma interna significa que para cada unidade do output, encontre a diferença entre o valor verdadeiro y e o valor previsto pela rede y^, então eleve essa diferença ao quadrado para depois somar todos os quadrados.\n",
    "![](imagens/sqe3.png)\n",
    "\n",
    "###  Gradiente Descendente:\n",
    "\n",
    "Para calcular a taxa de mudança, nos voltamos para o cálculo, em específico, para as derivadas. A derivada de uma função f(x) te retorna outra função f’(x) que retorna a inclinação de f(x) no ponto x. Por exemplo, considere f(x)=x^2. A derivada de x^2 é f’(x) = 2x. Então, em x = 2, a inclinação é f’(2) = 4. Colocando isso em um gráfico, temos a seguinte figura:\n",
    "![](imagens/gradiente1.png)\n",
    "\n",
    "Uma vez que os pesos simplesmente vão onde quer que o gradiente os leve, eles podem terminar em um lugar onde o erro é pequeno, mas não o menor possível. Esses pontos são chamados de mínimos locais. Caso os pesos sejam inicializados com o valor errado, o gradiente descendente pode os levar para mínimos locais, tal como ilustrado abaixo. \n",
    "![](imagens/gradiente2.png)\n",
    "\n",
    "A ideia por traz dessa técnica é ir minimizando o erro comforme a figura abaixo:\n",
    "![](imagens/gradiente3.png)\n",
    "\n",
    "O vídeo Gradiente de descida: a matemática explica como as formulas abaixo foram deduzidas\n",
    "![](imagens/gradiente4.png)\n",
    "![](imagens/gradiente5.png)\n",
    "![](imagens/gradiente6.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de descida: o código\n",
    "Como vimos antes, a atualização de um peso pode ser calculada da seguinte maneira:\n",
    "![](imagens/gradiente7.png)\n",
    "\n",
    "\n",
    "com o termo para erro \\deltaδ como\n",
    "![](imagens/gradiente8.png)\n",
    "\n",
    "Lembre-se, na equação acima (y - y^) é o erro do output, e f'(h) se refere à derivada da função de ativação, f(h). Podemos chamar essa derivada de gradiente do output.\n",
    "\n",
    "Agora, irei escrever isso em código levando em conta o caso de apenas uma unidade de output. Também usarei a função sigmóide como função de ativação f(h).\n",
    "![](imagens/codigo1.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.5\n",
      "Amount of Error:\n",
      "0.0\n",
      "Change in Weights:\n",
      "[-0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "#x = np.array([1, 2, 3, 4])\n",
    "#x = np.array([-0.02031869, -0.04063738, -0.06095608, -0.08127477])\n",
    "#x= np.array([ -1.03203226e-05,  -2.06406453e-05,  -3.09609730e-05,  -4.12812956e-05])\n",
    "#x = np.array([ -2.66272712e-12,  -5.32545426e-12,  -7.98818270e-12,  -1.06509098e-11])\n",
    "x = np.array([ -1.77225446e-25,  -3.54450894e-25,  -5.31676428e-25,  -7.08901873e-25])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Média do erro quadrado\n",
    "\n",
    "Iremos fazer uma pequena mudança no modo como calculamos o erro aqui. Em vez de usarmos o SQE, usaremos a média dos erros quadrados (MEQ). Agora que estamos usando muitos dados, somar todos os pesos e todos os passos pode criar passos muito grandes, que fazem o gradiente descendente divergir. Para compensar isso, será necessário usar uma taxa de aprendizagem muito pequena. Em vez disso, nós dividiremos pelo número de dados observados, mm para tirar a média. Desse modo, não importa quantos dados usemos, as taxas de aprendizado estarão tipicamente no intervalo entre 0,01 e 0,001. Então, podemos usar a MEQ (abaixo) para calcular o gradiente e o resultado da mesma forma que antes, porém, em um valor médio, em vez de uma somatória.\n",
    "![](imagens/meq1.png)\n",
    "\n",
    "Lembre-se que estamos usando a função de ativação sigmóide,\n",
    "![](imagens/meq2.png)\n",
    "\n",
    "E que o gradiente de uma sigmóide é:\n",
    "![](imagens/meq3.png)\n",
    "\n",
    "onde h é o input da unidade de output:\n",
    "![](imagens/meq4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando com Numpy\n",
    "\n",
    "Em grande parte, isso é bem simples de fazer com Numpy.\n",
    "\n",
    "Primeiro, é necessário inicializar os pesos. Queremos que eles sejam tão pequenos quanto o input para uma função sigmóide é na região linear próxima ao 0 e não espremido nas pontas alta e baixa. Também é importante inicializar-los aleatoriamente de modo que todos comecem de lugares diferentes e divirjam, quebrando simetrias. Portanto, inicializamos os pesos de uma distribuição normal com centro em 0. Um bom valor para essa escala é 1/\\sqrt(n) onde n é o número de unidades de input. Isso mantém o input da sigmóide baixo para números de unidades de input maiores.\n",
    "\n",
    "![](imagens/weights.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.26276093849966364\n",
      "Train loss:  0.20928619409324895\n",
      "Train loss:  0.20084292908073417\n",
      "Train loss:  0.1986215647552789\n",
      "Train loss:  0.19779851396686018\n",
      "Train loss:  0.19742577912189863\n",
      "Train loss:  0.19723507746241065\n",
      "Train loss:  0.19712945625092465\n",
      "Train loss:  0.19706766341315077\n",
      "Train loss:  0.19703005801777368\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep_13 import features, targets, features_test, targets_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        # Derivada da sigmoide\n",
    "        error_term = error *  output * (1 - output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += (learnrate * error_term) * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz de programação\n",
    "A seguir, você implementará uma rede 4x3x2 orientada a frente, com funções de ativação sigmóide em ambas as camadas.\n",
    "![](imagens/multilayer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49671415 -0.1382643   0.64768854  1.52302986]\n",
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "print(X)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in  = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in  = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Agora chegamos ao problema de como fazer uma rede neural de múltiplas camadas aprender. Antes, vimos como atualizar os pesos usando o gradiente descendente. O algoritmo de Retropropagação (backpropagation daqui em diante) é apenas uma extensão disso, usando a regra de cadeia para encontrar o erro respeitando os pesos conectando a camada de input para a camada oculta (numa rede de duas camadas).\n",
    "\n",
    "Para atualizar os pesos das camadas ocultas usando o gradiente descendente, é necessário saber quanto contribuiu cada unidade oculta para a produção daquele erro no output final. Uma vez que o output de uma camada é determinado pelos pesos entre camadas, o erro resultante de unidades é proporcional aos pesos ao longo da rede. Uma vez que sabemos o erro no output, nós usamos os pesos para trazê-lo de volta às camadas ocultas.\n",
    "\n",
    "#### Exemplo\n",
    "Detalharemos os passos de calcular as atualizações dos pesos de uma rede simples de duas camadas. Vamos assumir que temos dois valores de input, uma unidade oculta e uma unidade de output, com ativações em sigmóide nas camadas oculta e de output. A imagem seguinte desenha essa rede. (Nota: os valores de input são mostrados como nós no final da página, enquanto o valor de output da rede são mostrados como y^\n",
    "![](imagens/back1.png)\n",
    "![](imagens/back2.png)\n",
    "\n",
    "A partir deste exemplo, pudemos observar um dos efeitos de usar a função sigmóide para as ativações. A máxima derivada da função sigmóide é 0.25, então os erros da camada de output são reduzidos por no mínimo 75%, e os erros da camada oculta são reduzidos em até 93,75%! Fica claro que caso tenha muitas camadas, usar a função de ativação sigmóide irá reduzir rapidamente as atualizações de peso para valores ínfimos em camadas mais próximas do input. Isso é conhecido como o problema do gradiente de fuga. Mais adiante no curso veremos outras funções de ativação que são mais eficientes neste aspecto e são mais comumente usadas nas arquiteturas modernas de redes.\n",
    "\n",
    "#### Exercício de Backpropagation\n",
    "Abaixo, você implementará o código para calcular uma rodada de atualização com backpropagation para dois conjuntos de pesos. Escrevi o andamento para frente, o seu objetivo é escrever o andamento para trás.\n",
    "\n",
    "Coisas a fazer\n",
    "\n",
    "  -  Calcular o erro da rede.\n",
    "  -  Calcular o gradiente de erro da camada de output.\n",
    "  -  Usar a backpropagation para calcular o erro da camada oculta.\n",
    "  -  Calcular o passo de atualização dos pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = None\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = None\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = None\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = None\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = None\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imagens/resposta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício de Backpropagation\n",
    "\n",
    "Agora você implementará o algoritmo de backpropagation em uma rede treinada com os dados de admissão em universidades. Você possui tudo o que precisa para resolver esse exercício nos exercícios anteriores.\n",
    "\n",
    "Seus objetivos aqui:\n",
    "\n",
    " - Implementar o andamento adiante.\n",
    " - Implementar o algoritmo de backpropagation.\n",
    " - Atualizar os pesos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep_16 import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = None\n",
    "        hidden_output = None\n",
    "        output = None\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = None\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = None\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = None\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = None\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += 0\n",
    "        del_w_input_hidden += 0\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += 0\n",
    "    weights_hidden_output += 0\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
